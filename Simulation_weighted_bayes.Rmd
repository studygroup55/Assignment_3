---
title: "Weighted bayes"
author: "Lina Elkj√¶r Pedersen"
date: "2023-04-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#install.packages("rlist")

pacman::p_load(tidyverse, brms, lme4, rlist, rstan, cmdstanr)

```

*Simulation*

```{r}
set.seed <- 1981 # Defining a seed so the results are always the same

df = data.frame()


SimulateData <- function(n_agents, n_trials) {
  for (i in 1:n_agents) {
    FirstRating_list = list()
    SecondRating_list = list()
    Feedback_list = list()
    GroupRating_list = list()
    Agent_list = list()
    Trial_list = list()
    
    bias = 0
    w <- rbeta(1, 4, 2)
    print(w)
    
    #print(i)
    for (j in 1:n_trials) {
      #print(j)
      FirstRating <- round(runif(1, 1, 8), 0) #Creates a rating between 1 and 8
      Feedback <- round(runif(1, -3, 3), 0) # A feedback between -3 and 3
      while (Feedback == 1 | Feedback == -1){ #While feedback is 1 or -1, redo it
        Feedback <- round(runif(1, -3, 3), 0)
      } 
      
      GroupRating <- FirstRating + Feedback #Other is the rating of the self + the feedback
      while (GroupRating < 1 | GroupRating > 8) { #While Other is outside of range of 1 and 8, redo Feedback and recalculate Other
        Feedback <- round(runif(1, -3, 3), 0)
        while (Feedback == 1 | Feedback == -1){
          Feedback <- round(runif(1, -3, 3), 0)
        } 
        GroupRating <- FirstRating + Feedback
      }
      
      #Transforming 
      FirstRating_transformed = (((FirstRating - 1)/7)*0.8) + 0.1 #Transforming to space between 0.1 and 0.9
      GroupRating_transformed = (((GroupRating - 1)/7)*0.8) + 0.1
      
      #Calculating next choice
      SecondRating_transformed <- inv_logit_scaled(bias + w*logit_scaled(FirstRating_transformed) + (1-w)*logit_scaled(GroupRating_transformed)) #Simple bayes. Same weight so still a simple Bayes, but we want to take an average between the two sources of evidence. Otherwise we will put too much weight on the evidence. NB bias is not transformed because we defined it on this scale already.
      
      SecondRating = round((((SecondRating_transformed - 0.1)/0.8)*7) + 1) #Transforming SecondRating back
      
      FirstRating_list = list.append(FirstRating_list, FirstRating) #Lists for df
      SecondRating_list = list.append(SecondRating_list, SecondRating)
      Feedback_list = list.append(Feedback_list, Feedback)
      GroupRating_list = list.append(GroupRating_list, GroupRating)
      Agent_list = list.append(Agent_list, i)
      Trial_list = list.append(Trial_list, j)
    }
    
    df_temp <- data.frame(ID = unlist(Agent_list), Trial = unlist(Trial_list), FirstRating = unlist(FirstRating_list), Feedback = unlist(Feedback_list), SecondRating = unlist(SecondRating_list), GroupRating = unlist(GroupRating_list)) #Creating df by unpacking lists
    
    df <- rbind(df, df_temp)
    }

  return(df)
  }


n_agents = 10
n_trials = 500

df <- SimulateData(n_agents, n_trials) 

```


```{r}
#Adding a collumn for change 
df <-  df %>% 
  mutate(Change = SecondRating - FirstRating)

for (i in 1:n_agents) { 
  
  df_temp <- df %>% 
    subset(ID == i)
  
  data_weightedBayes <- list(
    N = nrow(df_temp),
    SecondRating = df_temp$SecondRating,
    FirstRating = df_temp$FirstRating,
    GroupRating = df_temp$GroupRating
  )

  mod_weightedBayes <- cmdstan_model("weighted_bayes.stan", cpp_options = list(stan_threads = TRUE),
                     stanc_options = list("O1"))



  samples <- mod_weightedBayes$sample(
    data = data_weightedBayes, # the data :-)
    seed = 123,  # a seed, so I always get the same results
    chains = 2,  # how many chains should I fit (to check whether they give the same results)
    parallel_chains = 2, # how many of the chains can be run in parallel?
    threads_per_chain = 2, # distribute gradient estimations within chain across multiple cores
    iter_warmup = 1000,  # warmup iterations through which hyperparameters (steps and step length) are adjusted
    iter_sampling = 2000, # total number of iterations
    refresh = 0,  # how often to show that iterations have been run
    max_treedepth = 20, # how many steps in the future to check to avoid u-turns
    adapt_delta = 0.99, # how high a learning rate to adjust hyperparameters during warmup
  )
  
  #Disabling scientific notation
  options(scipen=999)
    
  #extract summary 
  samples$summary()
  
  # assign function within loop
  assign(paste0("weighted_summary_ID_", i), samples$summary())
  
    
}
```



```{r}

#define range
p = seq(0, 1, length=100)

#create plot of Beta distribution with shape parameters 2 and 10
plot(p, dbeta(p, 4, 2), type='l')
```

